# 一、DNN

## 1.1 激活函数sigmoid，tanh，relu. 各自的优点和适用场景？



## 1.2 讲一下你了解的优化方法，sgd, momentum, rmsprop, adam的区别和联系？



## 1.3 讲一下你怎么理解dropout，分别从bagging和正则化的角度？

​        dropout是神经网络训练过程中按设定的比例随机的丢掉一些神经元，不参与训练(前向传播和反向传播)，是神经网络为了预防过拟合的一种重要的手段。

​		从概念上去理解dropout防止过拟合的原因，可以把**units的个数看成是模型的复杂度**，units个数越多，那么模型越复杂，过于复杂的模型会带来过拟合，那么相反地，units的个数被我们减少了，那么就能防止过度训练。

​       从另一个角度上说，每次随机的“删除”一些units,相当于得到了**不同的网络结构**，不同的units进行合作得到训练参数，这有点类似于bagging的过程。然而最后用作预测的时候，又会使用全部的units，相当于组合了所有的模型，因此dropout过程防止了训练中出现的参数拟合过度问题，同时还能组合训练得到的模型，从而获得一个更好的组合模型。

## 1.4 讲一下batch normalization

https://zhuanlan.zhihu.com/p/34879333

## 1.5 梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？

​       梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。具体从两个角度去看：

（1）深层网络角度

​		根据链式求导法则，更新梯度信息，如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生**梯度爆炸**，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了**梯度消失**。

（2）激活函数角度

sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失；

tanh作为激活函数，比sigmoid要好一些，但是它的导数仍然是小于1的。

解决办法：

- 预训练加微调
- 梯度剪切、权重正则（针对梯度爆炸）
- 使用不同的激活函数
- 使用batchnorm
- 使用残差结构
- 使用LSTM网络

参考

“详解机器学习中的梯度消失、爆炸原因及其解决方法”

https://blog.csdn.net/qq_25737169/article/details/78847691

"神经网络训练中的梯度消失与梯度爆炸"

https://zhuanlan.zhihu.com/p/25631496



# 二、CNN

说一下你理解的卷积核， 1x1的卷积核有什么作用？

讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？

讲一下AlexNet的具体结构，每层的作用



# 三、RNN

手推RNN和LSTM结构

LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失



# 四、工程方面

## 4.1 如果训练的神经网络不收敛，可能有哪些原因？

### I. 数据集问题

**数据集中是否有太多的噪音？**

我曾经遇到过这种情况，当我从一个食品网站抓取一个图像数据集时，错误标签太多以至于网络无法学习。手动检查一些输入样本并查看标签是否大致正确。

**检查你的输入数据**

检查馈送到网络的输入数据是否正确。例如，我不止一次混淆了图像的宽度和高度。有时，我错误地令输入数据全部为零，或者一遍遍地使用同一批数据执行梯度下降。因此打印／显示若干批量的输入和目标输出，并确保它们正确。

**输入与输出之间的关系是否太随机？**

相较于随机的部分（可以认为股票价格也是这种情况），输入与输出之间的非随机部分也许太小，即输入与输出的关联度太低。没有一个统一的方法来检测它，因为这要看数据的性质。

### II. 数据归一化/增强

**归一化特征**

你的输入已经归一化到零均值和单位方差了吗？

**检查你的预训练模型的预处理过程**

如果你正在使用一个已经预训练过的模型，确保你现在正在使用的**归一化和预处理与之前训练模型时的情况相同**。例如，一个图像像素应该在 [0, 1]，[-1, 1] 或 [0, 255] 的范围内吗？

### III. 实现的问题

**监控其它指标**

有时损失并不是衡量你的网络是否被正确训练的最佳预测器。如果可以的话，使用其它指标来帮助你，比如精度。

### IV. 训练问题

**增加、减少学习速率**

- 低学习速率将会导致你的模型收敛很慢；
- 高学习速率将会在开始阶段减少你的损失，但是可能会导致你很难找到一个好的解决方案。
- 试着把你当前的学习速率乘以 0.1 或 10。

**检查权重初始化**

如果不确定，请使用 Xavier 或 He 初始化。同样，初始化也许会给你带来坏的局部最小值，因此尝试不同的初始化，看看是否有效。









batch size对收敛速度的影响。