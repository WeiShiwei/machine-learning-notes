# 1 决策树的基本原理

​		决策树是一种分而治之(Divide and Conquer)的决策过程。一个困难的预测问题, 通过树的分支节点, 被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去(Recursive Partitioning)。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则(Stopping Rule)时, 该分支节点会停止劈分，此为自上而下的停止阈值(Cutoff Threshold)法；有些决策树也使用自下而上的剪枝(Pruning)法。



# 2 决策树的三要素

一棵决策树的生成过程主要分为以下3个部分:  

**特征选择**：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。 

**决策树生成**：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。树结构来说，递归结构是最容易理解的方式。 

**剪枝**：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。



# 2 特征选择

## 2.1 熵的定义

![1560161538564](.\images\1560161538564.png)

## 2.2 信息增益

信息增益的定义

![1560161631790](.\images\1560161631790.png)



信息增益的计算

![1560161665703](.\images\1560161665703.png)



## 2.3 信息增益比

![1560161770640](.\images\1560161770640.png)



## 2.4 基尼指数

CART则采用了基尼指数来划分属性，数据集D的纯度可以用基尼值来度量：

基尼值反应了从数据集中随机抽取两个样本，其类标签不一致的概率，因此基尼值越小，数据集的纯度越高。

![1560163068509](.\images\1560163068509.png)

![1560163101279](.\images\1560163101279.png)



# 3 决策树生成

ID3算法用信息增益选择特征

C4.5用信息增益比选择特征

## 3.1  C4.5算法

![1560162490004](.\images\1560162490004.png)



## 3.2 CART

​		CART树的构成就是递归的构建二叉树的过程，对回归树用**平方差最小化**准则，对分类树用**基尼指数最小化**准则，进行特征特征选择，生成二叉树

### 3.2.1 回归树

最小二乘回归树生成算法

![1561109927367](.\images\1561109927367.png)

### 3.2.2 分类树

TODO



# 4 决策树的剪枝

​		剪枝是解决过拟合问题的重要手段，主要分为“预剪枝”和“后剪枝”两种。在剪枝的时候我们要引入验证集用来帮助我们判断是否需要剪枝。

## 4.1 预剪枝

​		预剪枝是边生成决策树边剪枝的一种做法。基于信息增益准则或者增益率准则或者基尼指数，我们会选出最优的特征来进行数据集的划分，这个时候预剪枝做的就是判断划分前后，验证集的精度是否会提高，如果提高的话就进行划分，否则不进行划分，也就是剪枝了。 
预剪枝可以降低过拟合的风险，而且还显著减少了决策树的训练时间开销和测试时间开销。 
不过，预剪枝是一种贪心的做法，有些划分可能在当前不能提高性能，但在之后的划分中可以显著提高决策树的性能，所以**预剪枝有着欠拟合的风险**。

## 4.2 后剪枝

​		后剪枝是先生成一颗完整的决策树，然后自底向上地进行剪枝，判断某个分支结点替换为叶子结点后是否会提高验证集的精度，可以提高则将分支结点替换为叶子结点，否则不替换。 

​		后剪枝比预剪枝保留了更多的分支，**欠拟合的风险很小，泛化性能也往往优于预剪枝**。但后剪枝的训练**时间开销要比预剪枝大得多**。



## 4.3 TODO:形式化算法描述



# 5、决策数据的优缺点

树模型的优缺点。

## 5.1 优点

- **容易解释**
- 可以相对快地构建
- 可以**自然地处理连续和分类数据**
- **可以自然地处理缺失数据**
- **对输入中的异常值是稳健的**
- 在输入单调变换时是不变的
- 会执行隐式的变量选择
- **可以得到数据中的非线性关系**
- 可以得到输入之间的高阶交互
- 能很好地扩展到大型数据集



## 5.2 缺点

- 往往会选择具有更高数量的不同值的预测器
- 当预测器具有很多类别时，可能会过拟合
- **不稳定，有很高的方差**
- **缺乏平滑**
- 难以获取叠加结构
- 预测性能往往有限





# 面试题

## 面试问题1：什么是决策树？

**答：**决策树是一种分类和回归的基本模型，可从三个角度来理解它，即：

- 一棵树
- **if-then规则的集合**，该集合是决策树上的所有从根节点到叶节点的路径的集合
- **定义在特征空间与类空间上的条件概率分布**，决策树实际上是将特征空间划分成了互不相交的单元，每个从根到叶的路径对应着一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。实际中，哪个类别有较高的条件概率，就把该单元中的实例强行划分为该类别。



