## #3 背景知识

3.1 统计语言模型

3.2 n-gram模型

3.3 神经概率语言模型，NNLM

3.4 词向量的理解



## \#4 基于hierarchical softmax的模型

理解CBOW/Skip-gram模型，网络结构示意图

层次softmax与负采样的两种实现框架

目标函数

### 4.1 CBOW模型

#### 4.1.1 网络结构

与NNLM模型的不同：

- 从输入层到投影层的映射，NNLM是通过拼接，CBOW是通过累加求和

- NNLM有隐藏层，CBOW没有

- 输出层NNLM是线性结构，CBOW是树形结构

  

c

#### 4.1.2 梯度计算

相关记号及示意图

条件概率p(w|Context(w))，视为一系列二分类问题

目标函数及对数似然函数表达式

梯度计算，对theta，对xw，然后链式法则传递给v(w')，其中w' in Context(w)

伪代码



#### **问题1、层次softmax相比于NNLM，解决了什么问题，怎样解决的？**

层次softmax提升了性能

NNLM需要矩阵的乘法，softmax归一化计算，复杂度至少是O(N)

层次softmax只需要计算从树的根节点到叶节点，复杂度应该是O(logN)



### 4.2 Skip-gram模型

#### 4.2.1 网络结构

#### 4.2.2 梯度计算

- 条件概率

- 对数似然函数

- 梯度计算与更新

  

## \#5 基于Negative Sampling的模型

与层次softmax相比，NEG不使用复杂的霍夫曼树，而是利用简单的随机负采样，大幅度提高性能，是层次softmax的一种替代。

什么是正样本、负样本？

5.1 





问题1、层次softmax与负采样的区别和联系

为什么有softmax了，还需要负采样