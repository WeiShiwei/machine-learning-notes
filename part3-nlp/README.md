词向量

1、word2vec、glove、fasttext

2、elmo √

3、attention √

4、transformer

- self-attention
- multi-head self-attention
- 位置编码
- encoder 
- decoder(TODO)

代码实现

resiual connection/mask/layer norm/position encoding

5、GPT/Bert(TODO)

**transformer->GPT和BERT**

**GPT的单向和BERT的双向怎样理解？**

**预训练过程和fine-tunning**

**多种下游语言任务**





阅读

The Illustrated Transformer

<https://jalammar.github.io/illustrated-transformer/>

<https://blog.csdn.net/qq_41664845/article/details/84969266>

The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)

<http://jalammar.github.io/illustrated-bert/>

<https://blog.csdn.net/qq_41664845/article/details/84787969



实践：

Bidirectional RNNs

